name: Security Scanning

on:
  pull_request:
    branches: [ main, develop ]

permissions:
  contents: read
  pull-requests: write
  security-events: write

jobs:
  semgrep:
    name: Semgrep SAST
    runs-on: ubuntu-latest
    container:
      image: returntocorp/semgrep
    steps:
      - uses: actions/checkout@v4
      
      - name: Run Semgrep
        run: |
          semgrep scan \
            --config p/csharp \
            --config p/dotnet \
            --config p/owasp-top-ten \
            --config p/security-audit \
            --config p/sql \
            --config p/python \
            --config p/bash \
            --config p/ruby \
            --config p/rust \
            --json \
            --output=semgrep.json || true
        
      - name: Process Results
        run: |
          python3 << 'EOF'
          import json
          import os
          
          findings = {'critical': 0, 'high': 0, 'medium': 0, 'low': 0, 'info': 0}
          details = []
          
          try:
              with open('semgrep.json', 'r') as f:
                  data = json.load(f)
              
              for result in data.get('results', []):
                  severity = result.get('extra', {}).get('severity', 'INFO').upper()
                  rule_id = result.get('check_id', 'unknown')
                  message = result.get('extra', {}).get('message', 'No description')
                  path = result.get('path', 'unknown')
                  line = result.get('start', {}).get('line', 0)
                  
                  if severity == 'ERROR':
                      findings['high'] += 1
                      sev_label = 'high'
                  elif severity == 'WARNING':
                      findings['medium'] += 1
                      sev_label = 'medium'
                  else:
                      findings['low'] += 1
                      sev_label = 'low'
                  
                  details.append({
                      'severity': sev_label,
                      'rule': rule_id,
                      'message': message,
                      'location': f"{path}:{line}"
                  })
          except:
              pass
          
          output = {
              'summary': findings,
              'details': details[:10]  # Limit to top 10 for comment brevity
          }
          
          with open('semgrep-summary.json', 'w') as f:
              json.dump(output, f)
          EOF
      
      - name: Upload summary
        uses: actions/upload-artifact@v4
        with:
          name: semgrep-summary
          path: semgrep-summary.json
          retention-days: 1

  sqlfluff:
    name: SQLFluff Analysis
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      
      - name: Check for SQL files
        id: check_sql
        run: |
          if find . -type f -name "*.sql" | grep -q .; then
            echo "has_sql_files=true" >> $GITHUB_OUTPUT
          else
            echo "has_sql_files=false" >> $GITHUB_OUTPUT
          fi
      
      - name: Set up Python
        if: steps.check_sql.outputs.has_sql_files == 'true'
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
      
      - name: Install SQLFluff
        if: steps.check_sql.outputs.has_sql_files == 'true'
        run: pip install sqlfluff sqlfluff-templater-dbt
      
      - name: Run SQLFluff
        if: steps.check_sql.outputs.has_sql_files == 'true'
        run: |
          sqlfluff lint . --format json --dialect tsql > sqlfluff-results.json || true
        continue-on-error: true
      
      - name: Process SQLFluff Results
        if: steps.check_sql.outputs.has_sql_files == 'true'
        run: |
          python3 << 'EOF'
          import json
          
          findings = {'critical': 0, 'high': 0, 'medium': 0, 'low': 0, 'info': 0}
          details = []
          
          try:
              with open('sqlfluff-results.json', 'r') as f:
                  data = json.load(f)
              
              for violation in data:
                  rule_code = violation.get('code', '')
                  description = violation.get('description', 'No description')
                  filepath = violation.get('filepath', 'unknown')
                  line = violation.get('line_no', 0)
                  
                  if rule_code.startswith('L0'):
                      findings['low'] += 1
                      sev_label = 'low'
                  else:
                      findings['medium'] += 1
                      sev_label = 'medium'
                  
                  details.append({
                      'severity': sev_label,
                      'rule': rule_code,
                      'message': description,
                      'location': f"{filepath}:{line}"
                  })
          except:
              pass
          
          output = {
              'summary': findings,
              'details': details[:10]
          }
          
          with open('sqlfluff-summary.json', 'w') as f:
              json.dump(output, f)
          EOF
      
      - name: Create empty summary if no SQL files
        if: steps.check_sql.outputs.has_sql_files == 'false'
        run: |
          echo '{"summary": {"critical": 0, "high": 0, "medium": 0, "low": 0, "info": 0}, "details": []}' > sqlfluff-summary.json
      
      - name: Upload summary
        uses: actions/upload-artifact@v4
        with:
          name: sqlfluff-summary
          path: sqlfluff-summary.json
          retention-days: 1

  tsqllint:
    name: TSQLLint Analysis
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      
      - name: Check for SQL files
        id: check_sql
        run: |
          if find . -type f -name "*.sql" | grep -q .; then
            echo "has_sql_files=true" >> $GITHUB_OUTPUT
          else
            echo "has_sql_files=false" >> $GITHUB_OUTPUT
          fi
      
      - name: Setup .NET
        if: steps.check_sql.outputs.has_sql_files == 'true'
        uses: actions/setup-dotnet@v4
        with:
          dotnet-version: '8.0.x'
      
      - name: Install TSQLLint
        if: steps.check_sql.outputs.has_sql_files == 'true'
        run: dotnet tool install --global TSQLLint
      
      - name: Run TSQLLint
        if: steps.check_sql.outputs.has_sql_files == 'true'
        run: |
          ~/.dotnet/tools/tsqllint . > tsqllint-results.txt || true
        continue-on-error: true
      
      - name: Process TSQLLint Results
        if: steps.check_sql.outputs.has_sql_files == 'true'
        run: |
          python3 << 'EOF'
          import re
          import json
          
          findings = {'critical': 0, 'high': 0, 'medium': 0, 'low': 0, 'info': 0}
          details = []
          
          try:
              with open('tsqllint-results.txt', 'r') as f:
                  lines = f.readlines()
              
              # TSQLLint format: filepath(line,col) : error/warning code : message
              pattern = r'(.+?)\((\d+),\d+\)\s*:\s*(error|warning)\s+(\S+)\s*:\s*(.+)'
              
              for line in lines:
                  match = re.match(pattern, line, re.IGNORECASE)
                  if match:
                      filepath, line_num, level, code, message = match.groups()
                      
                      if level.lower() == 'error':
                          findings['high'] += 1
                          sev_label = 'high'
                      else:
                          findings['medium'] += 1
                          sev_label = 'medium'
                      
                      details.append({
                          'severity': sev_label,
                          'rule': code,
                          'message': message.strip(),
                          'location': f"{filepath}:{line_num}"
                      })
          except:
              pass
          
          output = {
              'summary': findings,
              'details': details[:10]
          }
          
          with open('tsqllint-summary.json', 'w') as f:
              json.dump(output, f)
          EOF
      
      - name: Create empty summary if no SQL files
        if: steps.check_sql.outputs.has_sql_files == 'false'
        run: |
          echo '{"summary": {"critical": 0, "high": 0, "medium": 0, "low": 0, "info": 0}, "details": []}' > tsqllint-summary.json
      
      - name: Upload summary
        uses: actions/upload-artifact@v4
        with:
          name: tsqllint-summary
          path: tsqllint-summary.json
          retention-days: 1

  powershell:
    name: PowerShell Analysis
    runs-on: windows-latest
    steps:
      - uses: actions/checkout@v4
      
      - name: Check for PowerShell files
        id: check_ps
        shell: pwsh
        run: |
          $psFiles = Get-ChildItem -Path . -Recurse -Include *.ps1,*.psm1,*.psd1 -ErrorAction SilentlyContinue
          if ($psFiles) {
            echo "has_ps_files=true" >> $env:GITHUB_OUTPUT
          } else {
            echo "has_ps_files=false" >> $env:GITHUB_OUTPUT
          }
      
      - name: Run PSScriptAnalyzer
        if: steps.check_ps.outputs.has_ps_files == 'true'
        shell: pwsh
        run: |
          Install-Module -Name PSScriptAnalyzer -Force -Scope CurrentUser
          $results = Invoke-ScriptAnalyzer -Path . -Recurse -Severity Error,Warning,Information
          
          $findings = @{
            critical = 0
            high = 0
            medium = 0
            low = 0
            info = 0
          }
          
          $details = @()
          
          foreach ($result in $results) {
            $sevLabel = switch ($result.Severity) {
              'Error' { $findings.high++; 'high' }
              'Warning' { $findings.medium++; 'medium' }
              'Information' { $findings.low++; 'low' }
            }
            
            $details += @{
              severity = $sevLabel
              rule = $result.RuleName
              message = $result.Message
              location = "$($result.ScriptPath):$($result.Line)"
            }
          }
          
          $output = @{
            summary = $findings
            details = $details | Select-Object -First 10
          }
          
          $output | ConvertTo-Json -Depth 3 | Out-File -FilePath powershell-summary.json
      
      - name: Create empty summary if no PS files
        if: steps.check_ps.outputs.has_ps_files == 'false'
        shell: pwsh
        run: |
          @{
            summary = @{
              critical = 0
              high = 0
              medium = 0
              low = 0
              info = 0
            }
            details = @()
          } | ConvertTo-Json | Out-File -FilePath powershell-summary.json
      
      - name: Upload summary
        uses: actions/upload-artifact@v4
        with:
          name: powershell-summary
          path: powershell-summary.json
          retention-days: 1

  checkov:
    name: Checkov IaC Scan
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      
      - name: Check for IaC files
        id: check_iac
        run: |
          if find . -type f \( -name "*.tf" -o -name "Dockerfile" -o -name "*.yaml" -o -name "*.yml" \) | grep -q .; then
            echo "has_iac_files=true" >> $GITHUB_OUTPUT
          else
            echo "has_iac_files=false" >> $GITHUB_OUTPUT
          fi
      
      - name: Set up Python
        if: steps.check_iac.outputs.has_iac_files == 'true'
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
      
      - name: Install Checkov
        if: steps.check_iac.outputs.has_iac_files == 'true'
        run: pip install checkov
      
      - name: Run Checkov
        if: steps.check_iac.outputs.has_iac_files == 'true'
        run: |
          checkov -d . --output json --output-file-path . --quiet || true
        continue-on-error: true
      
      - name: Process Checkov Results
        if: steps.check_iac.outputs.has_iac_files == 'true'
        run: |
          python3 << 'EOF'
          import json
          import os
          
          findings = {'critical': 0, 'high': 0, 'medium': 0, 'low': 0, 'info': 0}
          details = []
          
          try:
              # Checkov creates results_json.json
              with open('results_json.json', 'r') as f:
                  data = json.load(f)
              
              # Process failed checks
              for result in data.get('results', {}).get('failed_checks', []):
                  check_id = result.get('check_id', 'unknown')
                  check_name = result.get('check_name', 'No description')
                  file_path = result.get('file_path', 'unknown')
                  file_line = result.get('file_line_range', [0])[0]
                  guideline = result.get('guideline', '')
                  
                  # Checkov severity mapping
                  severity_map = {
                      'CRITICAL': 'critical',
                      'HIGH': 'high',
                      'MEDIUM': 'medium',
                      'LOW': 'low'
                  }
                  
                  # Checkov doesn't always provide severity, default to medium
                  severity = severity_map.get(result.get('severity', 'MEDIUM'), 'medium')
                  findings[severity] += 1
                  
                  details.append({
                      'severity': severity,
                      'rule': check_id,
                      'message': check_name,
                      'guideline': guideline,
                      'location': f"{file_path}:{file_line}"
                  })
          except Exception as e:
              print(f"Error processing Checkov results: {e}")
              pass
          
          # Sort by severity and limit to top 10
          severity_order = {'critical': 0, 'high': 1, 'medium': 2, 'low': 3, 'info': 4}
          details.sort(key=lambda x: severity_order.get(x['severity'], 5))
          
          output = {
              'summary': findings,
              'details': details[:10]
          }
          
          with open('checkov-summary.json', 'w') as f:
              json.dump(output, f)
          EOF
      
      - name: Create empty summary if no IaC files
        if: steps.check_iac.outputs.has_iac_files == 'false'
        run: |
          echo '{"summary": {"critical": 0, "high": 0, "medium": 0, "low": 0, "info": 0}, "details": []}' > checkov-summary.json
      
      - name: Upload summary
        uses: actions/upload-artifact@v4
        with:
          name: checkov-summary
          path: checkov-summary.json
          retention-days: 1

  trivy:
    name: Trivy Container Scan
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      
      - name: Run Trivy
        uses: aquasecurity/trivy-action@master
        with:
          scan-type: 'fs'
          scan-ref: '.'
          format: 'json'
          output: 'trivy-results.json'
          scanners: 'vuln,secret,config'
        continue-on-error: true
      
      - name: Process Results
        run: |
          python3 << 'EOF'
          import json
          
          findings = {'critical': 0, 'high': 0, 'medium': 0, 'low': 0, 'info': 0}
          details = []
          
          try:
              with open('trivy-results.json', 'r') as f:
                  data = json.load(f)
              
              for result in data.get('Results', []):
                  target = result.get('Target', 'unknown')
                  
                  # Process vulnerabilities
                  for vuln in result.get('Vulnerabilities', []):
                      severity = vuln.get('Severity', '').lower()
                      if severity in findings:
                          findings[severity] += 1
                      
                      vuln_id = vuln.get('VulnerabilityID', 'unknown')
                      pkg_name = vuln.get('PkgName', 'unknown')
                      installed_version = vuln.get('InstalledVersion', 'unknown')
                      fixed_version = vuln.get('FixedVersion', 'not available')
                      title = vuln.get('Title', 'No description')
                      
                      details.append({
                          'severity': severity,
                          'type': 'vulnerability',
                          'id': vuln_id,
                          'message': f"{pkg_name} {installed_version}: {title}",
                          'fix': f"Upgrade to {fixed_version}" if fixed_version != 'not available' else 'No fix available',
                          'location': target
                      })
                  
                  # Process misconfigurations
                  for misconfig in result.get('Misconfigurations', []):
                      severity = misconfig.get('Severity', '').lower()
                      if severity in findings:
                          findings[severity] += 1
                      
                      rule_id = misconfig.get('ID', 'unknown')
                      title = misconfig.get('Title', 'No description')
                      message = misconfig.get('Message', '')
                      
                      details.append({
                          'severity': severity,
                          'type': 'misconfiguration',
                          'id': rule_id,
                          'message': title,
                          'fix': message,
                          'location': target
                      })
          except:
              pass
          
          # Sort by severity (critical first) and limit to top 10
          severity_order = {'critical': 0, 'high': 1, 'medium': 2, 'low': 3, 'info': 4}
          details.sort(key=lambda x: severity_order.get(x['severity'], 5))
          
          output = {
              'summary': findings,
              'details': details[:10]
          }
          
          with open('trivy-summary.json', 'w') as f:
              json.dump(output, f)
          EOF
      
      - name: Upload summary
        uses: actions/upload-artifact@v4
        with:
          name: trivy-summary
          path: trivy-summary.json
          retention-days: 1

  trufflehog:
    name: TruffleHog Secrets
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0
      
      - name: Run TruffleHog
        uses: trufflesecurity/trufflehog@main
        with:
          extra_args: --json --only-verified
        continue-on-error: true
      
      - name: Process Results
        run: |
          python3 << 'EOF'
          import json
          
          findings = {'critical': 0, 'high': 0, 'medium': 0, 'low': 0, 'info': 0}
          details = []
          
          # TruffleHog outputs are difficult to capture from the action
          # This is a placeholder structure
          
          output = {
              'summary': findings,
              'details': details
          }
          
          with open('trufflehog-summary.json', 'w') as f:
              json.dump(output, f)
          EOF
      
      - name: Upload summary
        uses: actions/upload-artifact@v4
        with:
          name: trufflehog-summary
          path: trufflehog-summary.json
          retention-days: 1

  report:
    name: Post Results
    runs-on: ubuntu-latest
    needs: [semgrep, sqlfluff, tsqllint, powershell, checkov, trivy, trufflehog]
    if: always() && github.event_name == 'pull_request'
    steps:
      - name: Download all artifacts
        uses: actions/download-artifact@v4
      
      - name: Generate Report
        run: |
          python3 << 'EOF'
          import json
          
          def load_results(filename):
              try:
                  with open(filename, 'r') as f:
                      data = json.load(f)
                      # Handle both old format (dict) and new format (dict with summary/details)
                      if 'summary' in data:
                          return data
                      else:
                          return {'summary': data, 'details': []}
              except:
                  return {'summary': {'critical': 0, 'high': 0, 'medium': 0, 'low': 0, 'info': 0}, 'details': []}
          
          semgrep = load_results('semgrep-summary/semgrep-summary.json')
          sqlfluff = load_results('sqlfluff-summary/sqlfluff-summary.json')
          tsqllint = load_results('tsqllint-summary/tsqllint-summary.json')
          powershell = load_results('powershell-summary/powershell-summary.json')
          checkov = load_results('checkov-summary/checkov-summary.json')
          trivy = load_results('trivy-summary/trivy-summary.json')
          trufflehog = load_results('trufflehog-summary/trufflehog-summary.json')
          
          def severity_badge(count, severity):
              if count == 0:
                  return f"![{severity}](https://img.shields.io/badge/{severity}-0-success)"
              colors = {
                  'critical': 'critical',
                  'high': 'important',
                  'medium': 'yellow',
                  'low': 'blue',
                  'info': 'lightgrey'
              }
              return f"![{severity}](https://img.shields.io/badge/{severity}-{count}-{colors[severity]})"
          
          def has_findings(data):
              return sum(data['summary'].values()) > 0
          
          report = "## Security Scan Results\n\n"
          report += "| Tool | Critical | High | Medium | Low | Info |\n"
          report += "|------|----------|------|--------|-----|------|\n"
          
          tools = [
              ('Semgrep SAST', semgrep),
              ('SQLFluff', sqlfluff),
              ('TSQLLint', tsqllint),
              ('PowerShell', powershell),
              ('Checkov IaC', checkov),
              ('Trivy SCA', trivy),
              ('TruffleHog Secrets', trufflehog)
          ]
          
          for name, data in tools:
              summary = data['summary']
              report += f"| {name} | "
              report += " | ".join([severity_badge(summary[s], s) for s in ['critical', 'high', 'medium', 'low', 'info']])
              report += " |\n"
          
          total = {s: sum(tool[1]['summary'][s] for tool in tools) for s in ['critical', 'high', 'medium', 'low', 'info']}
          report += f"| **Total** | "
          report += " | ".join([severity_badge(total[s], s) for s in ['critical', 'high', 'medium', 'low', 'info']])
          report += " |\n"
          
          # Add detailed findings for each tool
          for name, data in tools:
              if has_findings(data) and data['details']:
                  report += f"\n### {name} - Top Findings\n"
                  
                  for detail in data['details']:
                      severity_emoji = {
                          'critical': 'ðŸ”´',
                          'high': 'ðŸŸ ',
                          'medium': 'ðŸŸ¡',
                          'low': 'ðŸ”µ',
                          'info': 'âšª'
                      }
                      emoji = severity_emoji.get(detail['severity'], 'âšª')
                      
                      if 'type' in detail:  # Trivy format
                          report += f"- {emoji} **{detail['id']}** ({detail['type']}): {detail['message']}\n"
                          report += f"  - Location: `{detail['location']}`\n"
                          if detail.get('fix'):
                              report += f"  - Fix: {detail['fix']}\n"
                      elif 'guideline' in detail:  # Checkov format
                          report += f"- {emoji} **{detail['rule']}**: {detail['message']}\n"
                          report += f"  - Location: `{detail['location']}`\n"
                          if detail.get('guideline'):
                              report += f"  - Guideline: {detail['guideline']}\n"
                      else:  # Standard format
                          report += f"- {emoji} **{detail['rule']}**: {detail['message']}\n"
                          report += f"  - Location: `{detail['location']}`\n"
                  
                  report += "\n"
          
          with open('report.md', 'w') as f:
              f.write(report)
          EOF
      
      - name: Comment PR
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            const report = fs.readFileSync('report.md', 'utf8');
            
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: report
            });
